代码倒挺简单的，总体来说就是neuronal_charge和neuronal_reset的过程中加了detach，切断了相应的反向传播通道

neuron dropout的时候好像self.mask没动过？——是动了的，forward函数里有调用

spiking_vgg.py里的replace和WrapedSNNOp是啥？
OnlineSpikingVGGF里的self.fb_conv那条线融合了上一个iter过完主干网络之后的feature，暂时不知道有啥用
WrapedSNNOp的反向传播：output -> out_for_grad -> in_for_grad -> rate -> spike。相当于rate替换掉spike进行反向传播——
这里的rate在neuron.py的OnlineLIFNode中产生，对应论文中的pL/pW的公式。

OTTT的实现方式应该还是和论文里不一致的。

charge -> fire -> reset，charge是自己的，fire和reset都是继承spikingjelly的

